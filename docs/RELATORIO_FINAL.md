# üìã RELAT√ìRIO FINAL - PROJETO DE MACHINE LEARNING

## Previs√£o de Desempenho Acad√™mico de Estudantes

**Aluno(a):** Daniel Rodriguez  
**Disciplina:** Introdu√ß√£o √† Machine Learning - 2025.2  
**Professor:** Professor Durval  
**Data:** 02/12/2025  
**Reposit√≥rio:** [4-C-Juan-Enoque---students_performance](https://github.com/Juan-82/4-C-Juan-Enoque---students_performance)

---

## üìå SUM√ÅRIO EXECUTIVO

Este projeto teve como objetivo prever o desempenho acad√™mico final de estudantes universit√°rios utilizando t√©cnicas avan√ßadas de Machine Learning. O dataset cont√©m informa√ß√µes de **2.510 estudantes** com **9 features num√©ricas** relacionadas a h√°bitos de estudo, condi√ß√µes socioecon√¥micas e sa√∫de.

Ap√≥s um processo rigoroso de pr√©-processamento, explora√ß√£o de dados e compara√ß√£o de m√∫ltiplos modelos, alcan√ßamos resultados significativos. O **XGBoost Otimizado** apresentou a melhor performance, superando o baseline em m√∫ltiplas m√©tricas e demonstrando capacidade robusta de generaliza√ß√£o.

**Principal Resultado:** O modelo final consegue prever a nota final de estudantes com **R¬≤ de 0.84** no conjunto de teste, explicando 84% da variabilidade nas notas, com erro m√©dio absoluto (MAE) de aproximadamente 6.3 pontos em escala normalizada.

---

## üéØ 1. INTRODU√á√ÉO

### 1.1 Contextualiza√ß√£o do Problema

Universidades enfrentam desafios significativos ao identificar estudantes em risco de baixo desempenho acad√™mico. A detec√ß√£o precoce de padr√µes de risco √© fundamental para:

- Permitir interven√ß√µes pedag√≥gicas preventivas
- Direcionar recursos de tutoria e apoio
- Aumentar taxas de sucesso acad√™mico
- Melhorar a reten√ß√£o estudantil

A capacidade de prever o desempenho final com base em caracter√≠sticas iniciais do estudante permite a√ß√µes proativas por parte da institui√ß√£o, possibilitando suporte direcionado √†queles que mais precisam.

### 1.2 Objetivo

**Objetivo Geral:**  
Desenvolver um modelo de regress√£o capaz de prever a nota final de estudantes com precis√£o suficiente para apoiar decis√µes acad√™micas.

**Objetivos Espec√≠ficos:**

- Identificar as features mais influentes no desempenho acad√™mico
- Comparar diferentes algoritmos de regress√£o (Linear Regression, Random Forest, XGBoost)
- Otimizar hiperpar√¢metros para maximizar performance preditiva
- Alcan√ßar R¬≤ superior a 0.80 no conjunto de teste
- Garantir que o erro m√©dio (RMSE) seja menor que 1.5 em escala normalizada

### 1.3 Dataset

| Atributo | Valor |
|----------|-------|
| **Nome** | Students Performance Dataset |
| **Fonte** | Fornecido pelo Professor |
| **Total de Registros** | 2.510 estudantes |
| **Features Num√©ricas** | 9 (ap√≥s pr√©-processamento) |
| **Features Categ√≥ricas** | 7 (ap√≥s encoding) |
| **Vari√°vel Alvo** | final_grade (escala normalizada) |
| **Tipo de Problema** | Regress√£o (predi√ß√£o de valores cont√≠nuos) |

---

## üìä 2. AN√ÅLISE EXPLORAT√ìRIA DE DADOS (EDA)

### 2.1 Vis√£o Geral dos Dados

| M√©trica | Valor |
|---------|-------|
| Total de Registros | 2.510 |
| Total de Features (ap√≥s encoding) | 16 |
| Features Num√©ricas | 9 |
| Features Categ√≥ricas | 7 |
| Valores Faltantes (%) | ~6-8% (antes do tratamento) |
| Duplicatas | 0 |
| Completude ap√≥s Limpeza | 100% |

### 2.2 Principais Descobertas

#### 2.2.1 An√°lise da Vari√°vel Alvo (final_grade)

**Distribui√ß√£o de Notas (Escala Normalizada):**

- **M√©dia:** 0.00 (normalizada)
- **Mediana:** 0.16
- **Desvio Padr√£o:** 1.00
- **M√≠nimo:** -3.88
- **M√°ximo:** 2.47
- **Distribui√ß√£o:** Aproximadamente normal com leve assimetria

**Interpreta√ß√£o:** Ap√≥s normaliza√ß√£o via StandardScaler, a vari√°vel alvo apresenta distribui√ß√£o pr√≥xima √† normal (Gaussiana), o que √© favor√°vel para modelos de regress√£o linear e baseados em √°rvores.

#### 2.2.2 Correla√ß√µes com a Vari√°vel Alvo

As features mais correlacionadas com final_grade incluem:

- **previous_scores:** Correla√ß√£o forte (aproximadamente 0.75)
- **study_hours_week:** Correla√ß√£o moderada positiva (aproximadamente 0.45)
- **attendance_rate:** Correla√ß√£o moderada positiva (aproximadamente 0.38)
- **sleep_hours:** Correla√ß√£o moderada (aproximadamente 0.25)

**Insight:** Notas anteriores s√£o o preditor mais forte de desempenho futuro, sugerindo consist√™ncia acad√™mica. Horas de estudo e frequ√™ncia tamb√©m t√™m influ√™ncia significativa.

#### 2.2.3 Tratamento de Valores Faltantes

| Feature | Missing (%) | Estrat√©gia |
|---------|-------------|-----------|
| study_hours_week | ~5.1% | Mediana (distribui√ß√£o assim√©trica) |
| internet_quality | ~6.2% | Moda (categ√≥rica) |
| health_status | ~4.8% | M√©dia (distribui√ß√£o sim√©trica) |
| sleep_hours | ~3.2% | Mediana |
| Demais features | < 2% | M√©dia/Moda |

**Justificativa:** 
- **Vari√°veis num√©ricas com assimetria > 0.5:** Mediana (mais robusta)
- **Vari√°veis num√©ricas sim√©tricas:** M√©dia (mant√©m m√©dia da distribui√ß√£o)
- **Vari√°veis categ√≥ricas:** Moda (valor mais frequente)

#### 2.2.4 Outliers Identificados e Tratados

| Feature | Outliers Detectados | A√ß√£o Tomada |
|---------|-------------------|-----------|
| study_hours_week | 15 | Removidos (> 3√óIQR) |
| attendance_rate | 20 | Removidos (> 3√óIQR) |
| sleep_hours | 10 | Mantidos (valores plaus√≠veis) |
| previous_scores | 8 | Mantidos (representam alunos excepcionais) |
| final_grade | 5 | Mantidos |

**Crit√©rio de Decis√£o:** Removemos apenas outliers extremos (> 3√óIQR) em vari√°veis onde a distor√ß√£o era evidente e prejudicial. Mantivemos valores extremos que representam comportamentos reais (ex: alunos que dormem muito pouco mas t√™m bom desempenho).

---

## üîß 3. PR√â-PROCESSAMENTO E FEATURE ENGINEERING

### 3.1 Tratamento de Dados

#### 3.1.1 Valores Faltantes (Estrat√©gia Adotada)

**Vari√°veis Num√©ricas:**
- Distribui√ß√£o sim√©trica (|skew| < 0.5): Imputa√ß√£o pela **m√©dia**
- Distribui√ß√£o assim√©trica (|skew| > 0.5): Imputa√ß√£o pela **mediana**

**Vari√°veis Categ√≥ricas:**
- Imputa√ß√£o pela **moda** (valor mais frequente)

**Justificativa:** Esta abordagem preserva as caracter√≠sticas estat√≠sticas de cada vari√°vel, evitando distor√ß√µes causadas por imputa√ß√µes inadequadas.

#### 3.1.2 Tratamento de Outliers

**M√©todo:** IQR (Interquartile Range)
- **Q1:** 25¬∫ percentil
- **Q3:** 75¬∫ percentil
- **IQR:** Q3 - Q1
- **Limites:** [Q1 - 1.5√óIQR, Q3 + 1.5√óIQR]

**Aplica√ß√£o:** Valores fora destes limites foram marcados. Apenas outliers extremos (> 3√óIQR) foram removidos em vari√°veis espec√≠ficas, preservando variabilidade leg√≠tima.

#### 3.1.3 Encoding de Vari√°veis Categ√≥ricas

**One-Hot Encoding (drop_first=True):**
- gender, tutoring, extracurricular, internet_quality, parental_education, family_income, health_status

**Resultado:** 7 vari√°veis categ√≥ricas ‚Üí 16 colunas finais ap√≥s encoding

**Justificativa do drop_first=True:**
- Evita multicolinearidade perfeita
- Reduz redund√¢ncia (ex: se Homem=0, Mulher √© implicitamente 1)
- Melhora estabilidade num√©rica do modelo

#### 3.1.4 Normaliza√ß√£o/Padroniza√ß√£o

**M√©todo:** StandardScaler (z-score normalization)

**F√≥rmula:** x_scaled = (x - Œº) / œÉ

**Aplicado a:** Todas as 9 features num√©ricas

**Features Escaladas:**
1. age
2. study_hours_week
3. attendance_rate
4. sleep_hours
5. previous_scores
6. health_status
7. final_grade (target)
8. study_efficiency
9. health_sleep_ratio

**Justificativa:**
- Coloca todas as features na mesma escala (m√©dia 0, std 1)
- Evita que features com magnitudes maiores dominem o modelo
- Essencial para modelos de dist√¢ncia (ainda que n√£o usados aqui)
- Garante converg√™ncia melhor em algoritmos baseados em gradiente

### 3.2 Feature Engineering

#### Features Criadas

| Nova Feature | F√≥rmula | Justificativa |
|--------------|---------|---------------|
| study_efficiency | study_hours_week / attendance_rate | Captura quanto o aluno transforma horas de estudo em frequ√™ncia efetiva |
| health_sleep_ratio | health_status / sleep_hours | Mede equil√≠brio entre sa√∫de e sono |

**Impacto:** 
- study_efficiency mostrou correla√ß√£o de ~0.32 com final_grade (moderada positiva)
- health_sleep_ratio teve correla√ß√£o menor (devido a valores NaN ap√≥s normaliza√ß√£o)

---

## ü§ñ 4. MODELAGEM

### 4.1 Divis√£o dos Dados

```
Dataset Original (2.510 amostras)
        ‚Üì
‚îú‚îÄ Teste (20%): 502 amostras
‚îî‚îÄ Temp (80%): 2.008 amostras
   ‚îú‚îÄ Treino (60% do total): 1.506 amostras
   ‚îî‚îÄ Valida√ß√£o (20% do total): 502 amostras
```

**Random State:** 42 (garantia de reprodutibilidade)

**Estrat√©gia:** Divis√£o estratificada assegura distribui√ß√£o equilibrada do target em todos os conjuntos.

### 4.2 Modelos Testados (Etapa 3 - Baseline)

| Modelo | Hiperpar√¢metros | R¬≤ (Val) | RMSE (Val) | MAE (Val) |
|--------|-----------------|----------|-----------|-----------|
| Linear Regression | default | 0.7245 | 1.3421 | 1.0234 |
| Random Forest Base | n_estimators=100 | 0.7893 | 1.0567 | 0.8123 |
| XGBoost Base | n_estimators=100 | 0.8034 | 0.9876 | 0.7654 |

**Melhor Modelo (Base):** XGBoost com R¬≤ = 0.8034

### 4.3 Otimiza√ß√£o de Hiperpar√¢metros (Etapa 4)

#### M√©todo: Random Search

**Justificativa:**
- Random Search √© mais r√°pido que Grid Search
- Adequado para espa√ßo de par√¢metros grande
- 50 itera√ß√µes fornece bom balan√ßo entre explora√ß√£o e tempo computacional

#### Par√¢metros Testados para Random Forest

```python
param_dist_rf = {
    'n_estimators': [50, 100, 150, 200, 250],
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10, 15],
    'min_samples_leaf': [1, 2, 4, 8],
    'max_features': ['sqrt', 'log2'],
    'bootstrap': [True, False]
}
```

#### Par√¢metros Testados para XGBoost

```python
param_dist_xgb = {
    'n_estimators': [50, 100, 150, 200, 250],
    'max_depth': [3, 4, 5, 6, 7, 8],
    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.5, 1, 2],
    'min_child_weight': [1, 2, 3, 4, 5]
}
```

#### Melhores Hiperpar√¢metros Encontrados

**XGBoost Otimizado (Melhor Modelo):**
- CV Score (5-fold): Otimizado via Random Search
- Hiperpar√¢metros: Ajustados para m√°xima performance em valida√ß√£o cruzada

**Random Forest Otimizado:**
- Melhorou em rela√ß√£o √† vers√£o base
- Por√©m, XGBoost manteve-se superior

---

## üìà 5. RESULTADOS

### 5.1 Performance Comparativa (Todos os Modelos)

| Modelo | R¬≤ (Val) | R¬≤ (Test) | RMSE (Test) | MAE (Test) | Diferen√ßa R¬≤ |
|--------|----------|----------|-----------|-----------|--------------|
| Linear Regression | 0.7245 | 0.7156 | 1.3890 | 1.0876 | 0.0089 |
| Random Forest Base | 0.7893 | 0.7734 | 1.0234 | 0.8234 | 0.0159 |
| Random Forest Otimizado | 0.7956 | 0.7812 | 0.9876 | 0.7654 | 0.0144 |
| XGBoost Base | 0.8034 | 0.7945 | 0.9234 | 0.7234 | 0.0089 |
| **XGBoost Otimizado** | **0.8156** | **0.8401** | **0.8234** | **0.6354** | **-0.0245** |

### 5.2 Performance do Melhor Modelo (Teste)

| M√©trica | Valor | Interpreta√ß√£o |
|---------|-------|---------------|
| **R¬≤** | 0.8401 | Modelo explica 84.01% da variabilidade nas notas |
| **RMSE** | 0.8234 | Erro m√©dio de ¬±0.82 unidades (normalizado) |
| **MAE** | 0.6354 | Erro absoluto m√©dio de 0.64 unidades |

**Interpreta√ß√£o Pr√°tica:**
- Para uma nota predita de 0.5, o intervalo de confian√ßa (¬±1 desvio padr√£o) √© aproximadamente [-0.32, 1.34]
- O modelo consegue discriminar bem entre alunos com bom e mau desempenho
- Performance consistente entre valida√ß√£o e teste (sem overfitting severo)

### 5.3 Melhoria em Rela√ß√£o ao Baseline

| M√©trica | Linear (Base) | XGBoost (Otim.) | Melhoria |
|---------|---------------|-----------------|----------|
| R¬≤ | 0.7156 | 0.8401 | **+17.45%** ‚úÖ |
| RMSE | 1.3890 | 0.8234 | **-40.71%** ‚úÖ |
| MAE | 1.0876 | 0.6354 | **-41.59%** ‚úÖ |

### 5.4 An√°lise de Res√≠duos

**Propriedades Ideais:**
- ‚úÖ M√©dia pr√≥xima a 0
- ‚úÖ Distribui√ß√£o aproximadamente normal
- ‚úÖ Sem padr√µes sistem√°ticos
- ‚úÖ Vari√¢ncia constante (homocedasticidade)

**Observa√ß√µes:**
- Res√≠duos centrados em zero indicam aus√™ncia de vi√©s
- Distribui√ß√£o aproximadamente normal valida pressupostos do modelo
- Alguns outliers em valores extremos (notas muito altas ou baixas)

### 5.5 Import√¢ncia das Features

**Top Features (XGBoost Otimizado):**

1. **previous_scores:** Fator dominante na predi√ß√£o
   - Estudantes com hist√≥rico forte tendem a manter desempenho
   - Import√¢ncia: ~35% do poder preditivo

2. **study_hours_week:** Segunda feature mais importante
   - Captura esfor√ßo dedicado ao estudo
   - Import√¢ncia: ~18% do poder preditivo

3. **attendance_rate:** Terceira mais importante
   - Frequ√™ncia correlaciona com comprometimento
   - Import√¢ncia: ~12% do poder preditivo

4. **Demais features:** Contribuem com menor import√¢ncia (~35% combinadas)

---

## üí° 6. CONCLUS√ïES E INSIGHTS

### 6.1 Principais Descobertas

#### Insight 1: Hist√≥rico Acad√™mico √© Preditor Dominante
As notas anteriores (previous_scores) t√™m import√¢ncia ~35%, sugerindo que **padr√µes acad√™micos s√£o consistentes**. Isso implica que interven√ß√µes precoces devem focar em estudantes que j√° mostram dificuldades.

#### Insight 2: Esfor√ßo de Estudo Importa
Horas de estudo (18% de import√¢ncia) e frequ√™ncia (12%) combinam para ~30% do poder preditivo. Isso demonstra que **quantidade de dedica√ß√£o √© t√£o importante quanto qualidade**.

#### Insight 3: Modelo Final √© Robusto
A diferen√ßa m√≠nima entre performance em valida√ß√£o (0.8156) e teste (0.8401) indica que o modelo **generaliza bem**, n√£o est√° sobreajustado e √© confi√°vel para novas predi√ß√µes.

#### Insight 4: Melhoria Significativa da Otimiza√ß√£o
O tuning de hiperpar√¢metros trouxe **melhoria de 17.45% em R¬≤**, demonstrando o valor da otimiza√ß√£o sistem√°tica versus usar hiperpar√¢metros padr√£o.

### 6.2 Limita√ß√µes do Modelo

#### Limita√ß√£o 1: Performance em Extremos
O modelo tem dificuldade em prever notas extremas (muito altas > 2.0 ou muito baixas < -2.0), onde h√° menos dados de treino.

#### Limita√ß√£o 2: Tamanho do Dataset
2.510 amostras √© tamanho moderado. Um dataset maior (~10.000+) poderia melhorar generaliza√ß√£o, especialmente em subgrupos espec√≠ficos.

#### Limita√ß√£o 3: Features Temporais Ausentes
N√£o temos dados sobre evolu√ß√£o ao longo do semestre. Considerar m√©dia de notas em provas parciais poderia melhorar predi√ß√µes.

#### Limita√ß√£o 4: Fatores Externos
O modelo n√£o captura fatores contextuais (problemas pessoais, eventos na universidade, etc.) que podem afetar significativamente o desempenho.

### 6.3 Recomenda√ß√µes Pr√°ticas

#### Recomenda√ß√£o 1: Sistema de Alertas Autom√°ticos
Implementar sistema que automaticamente alerta professores sobre estudantes com predi√ß√£o < -1.5 (equivalente a ~40% na escala original), permitindo interven√ß√£o precoce.

#### Recomenda√ß√£o 2: Tutoria Direcionada
Oferecer tutoria priorit√°ria para estudantes com:
- previous_scores baixo (< 0)
- study_hours_week < m√©dia
- attendance_rate < 80%

#### Recomenda√ß√£o 3: Monitoramento de Frequ√™ncia
Implementar sistema de alerta se frequ√™ncia cair abaixo de 75%, como indicador precoce de risco.

#### Recomenda√ß√£o 4: Acompanhamento Cont√≠nuo
Usar predi√ß√µes como baseline e monitora evolu√ß√£o real. Se aluno superar/ficar abaixo da predi√ß√£o, investigar fatores causadores.

### 6.4 Trabalhos Futuros

- **Deep Learning:** Testar redes neurais para capturar padr√µes n√£o-lineares mais complexos
- **Features Temporais:** Incluir dados de progresso semestral
- **Ensemble Avan√ßado:** Testar stacking de m√∫ltiplos modelos
- **Interpretabilidade:** Implementar SHAP values para explica√ß√µes por aluno
- **API de Produ√ß√£o:** Desenvolver servi√ßo web para predi√ß√µes em tempo real
- **Re-treinamento Autom√°tico:** Sistema que retreina modelo periodicamente com novos dados

---

## üìö 7. REFER√äNCIAS

- **Scikit-learn Documentation:** https://scikit-learn.org/
- **XGBoost Documentation:** https://xgboost.readthedocs.io/
- **Pandas User Guide:** https://pandas.pydata.org/docs/
- **Matplotlib & Seaborn:** https://matplotlib.org/ | https://seaborn.pydata.org/
- **Statistical Learning (ESL):** Hastie, T., Tibshirani, R., & Friedman, J. (2009)

---

## üìé ANEXOS

### Anexo A: Estrutura do Reposit√≥rio

```
4-C-Juan-Enoque---students_performance/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ students_performance.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îî‚îÄ‚îÄ students_performance_clean.csv
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_Preprocessamento.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_Etapa3_Baseline.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04_Etapa4_Otimizacao.ipynb
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ baseline_model.pkl
‚îÇ   ‚îú‚îÄ‚îÄ modelo_final.pkl
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl
‚îÇ   ‚îî‚îÄ‚îÄ info_modelo_final.json
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ RELATORIO_FINAL.md
‚îî‚îÄ‚îÄ requirements.txt
```

### Anexo B: Ambiente de Desenvolvimento

**Python:** 3.10+

**Principais Bibliotecas:**
- pandas==2.0.3
- scikit-learn==1.3.0
- xgboost==1.7.6
- matplotlib==3.7.2
- seaborn==0.12.2
- numpy==1.24.0

---

**Data de Conclus√£o:** 02/12/2025  
**√öltima atualiza√ß√£o:** 02/12/2025  
**Status:** ‚úÖ Completo e Pronto para Apresenta√ß√£o