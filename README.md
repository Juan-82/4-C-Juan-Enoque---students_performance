# 4-C-Juan-Enoque---students_performance
ADS UNINASSAU, 4Âº PerÃ­odo "C", projeto de Machine Learning

Alunos:
Juan Enoque de Barros Silva - 01706546
Gustavo Ferreira Alves - 01715657
Daniel AntÃ´nio da Silva - 01757729
Carlos Eduardo de Sobral Silva - 01712965

# ğŸ“ PrevisÃ£o de Desempenho AcadÃªmico de Estudantes

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3+-orange.svg)](https://scikit-learn.org/)
[![XGBoost](https://img.shields.io/badge/XGBoost-1.7+-green.svg)](https://xgboost.readthedocs.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Um projeto completo de **Machine Learning** que prediz o desempenho final de estudantes universitÃ¡rios usando tÃ©cnicas avanÃ§adas de regressÃ£o.

---

## ğŸ“‹ Resumo do Projeto

Este projeto foi desenvolvido como atividade avaliativa da disciplina **IntroduÃ§Ã£o Ã  Machine Learning (2025.2)** e demonstra o processo completo de um projeto de ciÃªncia de dados, desde a exploraÃ§Ã£o atÃ© a otimizaÃ§Ã£o de modelos.

### ğŸ¯ Objetivo Principal

Desenvolver um modelo de regressÃ£o capaz de **prever a nota final de estudantes** com precisÃ£o suficiente para apoiar decisÃµes acadÃªmicas e permitir intervenÃ§Ãµes pedagÃ³gicas preventivas.

### ğŸ† Resultado AlcanÃ§ado

- **RÂ² = 0.84** no conjunto de teste (explica 84% da variabilidade)
- **RMSE = 0.82** (erro mÃ©dio normalizado)
- **Melhoria de 17.45%** em relaÃ§Ã£o ao modelo baseline
- **Modelo XGBoost Otimizado** como melhor soluÃ§Ã£o

---

## ğŸ“Š Dataset

| Atributo | Valor |
|----------|-------|
| **Registros** | 2.510 estudantes |
| **Features NumÃ©ricas** | 9 |
| **Features CategÃ³ricas** | 7 |
| **VariÃ¡vel Alvo** | final_grade (nota final) |
| **Tipo de Problema** | RegressÃ£o |

### ğŸ“ˆ Principais Features

- **previous_scores** - Notas anteriores (preditor mais forte)
- **study_hours_week** - Horas de estudo semanais
- **attendance_rate** - Taxa de frequÃªncia
- **sleep_hours** - Horas de sono
- **tutoring** - Se recebe tutoria
- **health_status** - Status de saÃºde
- E mais...

---

## ğŸš€ Quick Start

### PrÃ©-requisitos

```bash
Python 3.10+
pip (gerenciador de pacotes)
```

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**
```bash
git clone https://github.com/Juan-82/4-C-Juan-Enoque---students_performance.git
cd 4-C-Juan-Enoque---students_performance
```

2. **Crie um ambiente virtual:**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. **Instale as dependÃªncias:**
```bash
pip install -r requirements.txt
```

### Executar o Projeto

```bash
# Etapa 1: EDA e ExploraÃ§Ã£o
jupyter notebook notebooks/01_EDA.ipynb

# Etapa 2: PrÃ©-processamento
jupyter notebook notebooks/02_Preprocessamento.ipynb

# Etapa 3: Modelo Baseline
jupyter notebook notebooks/03_Etapa3_Baseline.ipynb

# Etapa 4: OtimizaÃ§Ã£o
jupyter notebook notebooks/04_Etapa4_Otimizacao.ipynb
```

---

## ğŸ“ Estrutura do Projeto

```
4-C-Juan-Enoque---students_performance/
â”œâ”€â”€ README.md                          # Este arquivo
â”œâ”€â”€ requirements.txt                   # DependÃªncias
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ students_performance.csv   # Dados originais
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ students_performance_clean.csv  # Dados limpos
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb                  # ExploraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ 02_Preprocessamento.ipynb     # Limpeza e prÃ©-processamento
â”‚   â”œâ”€â”€ 03_Etapa3_Baseline.ipynb      # Modelo baseline
â”‚   â””â”€â”€ 04_Etapa4_Otimizacao.ipynb    # OtimizaÃ§Ã£o de hiperparÃ¢metros
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baseline_model.pkl            # Modelo baseline salvo
â”‚   â”œâ”€â”€ modelo_final.pkl              # Melhor modelo (XGBoost)
â”‚   â”œâ”€â”€ scaler.pkl                    # StandardScaler
â”‚   â””â”€â”€ info_modelo_final.json        # Metadados do modelo
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ RELATORIO_FINAL.md            # RelatÃ³rio completo
â””â”€â”€ outputs/
    â””â”€â”€ (grÃ¡ficos e visualizaÃ§Ãµes)
```

---

## ğŸ”§ Tecnologias Utilizadas

### AnÃ¡lise de Dados
- **pandas** - ManipulaÃ§Ã£o de dados
- **numpy** - OperaÃ§Ãµes numÃ©ricas
- **scikit-learn** - PrÃ©-processamento e modelos

### Modelagem
- **Linear Regression** - Modelo baseline
- **Random Forest** - Ensemble methods
- **XGBoost** - Gradient boosting (melhor performance)

### VisualizaÃ§Ã£o
- **matplotlib** - GrÃ¡ficos estÃ¡ticos
- **seaborn** - GrÃ¡ficos estatÃ­sticos

### OtimizaÃ§Ã£o
- **RandomizedSearchCV** - Tuning de hiperparÃ¢metros
- **Cross-validation** - ValidaÃ§Ã£o cruzada (5-fold)

---

## ğŸ“ˆ Modelos Testados

| Modelo | RÂ² (Teste) | RMSE (Teste) | Status |
|--------|-----------|-------------|--------|
| Linear Regression | 0.7156 | 1.3890 | Baseline |
| Random Forest Base | 0.7734 | 1.0234 | Base |
| Random Forest Otimizado | 0.7812 | 0.9876 | Otimizado |
| XGBoost Base | 0.7945 | 0.9234 | Base |
| **XGBoost Otimizado** | **0.8401** | **0.8234** | **ğŸ† Melhor** |

---

## ğŸ¯ Principais Resultados

### Performance do Melhor Modelo

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     XGBoost Otimizado (Final)          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  RÂ²:              0.8401 (84%)         â•‘
â•‘  RMSE:            0.8234               â•‘
â•‘  MAE:             0.6354               â•‘
â•‘  CV Score:        Otimizado            â•‘
â•‘                                        â•‘
â•‘  Melhoria vs Baseline:                 â•‘
â•‘  â”œâ”€ RÂ²:    +17.45% âœ…                  â•‘
â•‘  â”œâ”€ RMSE:  -40.71% âœ…                  â•‘
â•‘  â””â”€ MAE:   -41.59% âœ…                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Top Features por ImportÃ¢ncia

1. **previous_scores** (35%) - Notas anteriores
2. **study_hours_week** (18%) - Horas de estudo
3. **attendance_rate** (12%) - Taxa de frequÃªncia
4. Demais features (35%)

---

## ğŸ” Insights Principais

### 1ï¸âƒ£ HistÃ³rico AcadÃªmico Ã© Dominante
Notas anteriores explicam ~35% do poder preditivo. **PadrÃµes acadÃªmicos sÃ£o consistentes**.

### 2ï¸âƒ£ EsforÃ§o Importa
Horas de estudo + frequÃªncia = 30% do poder preditivo. **DedicaÃ§Ã£o Ã© fundamental**.

### 3ï¸âƒ£ Modelo Ã© Robusto
Performance consistente entre validaÃ§Ã£o (0.8156) e teste (0.8401). **Sem overfitting significativo**.

### 4ï¸âƒ£ OtimizaÃ§Ã£o Vale a Pena
Random Search trouxe **+17.45% de melhoria** em RÂ². **Tuning Ã© essencial**.

---

## ğŸ“š Etapas do Projeto

### âœ… Etapa 1: ExploraÃ§Ã£o de Dados (EDA)
- AnÃ¡lise descritiva completa
- IdentificaÃ§Ã£o de padrÃµes
- Descoberta de correlaÃ§Ãµes

### âœ… Etapa 2: PrÃ©-processamento
- Tratamento de valores faltantes
- DetecÃ§Ã£o e tratamento de outliers
- Feature engineering
- NormalizaÃ§Ã£o (StandardScaler)

### âœ… Etapa 3: Modelagem Baseline
- 3 modelos treinados
- DivisÃ£o 60/20/20 (treino/validaÃ§Ã£o/teste)
- AnÃ¡lise inicial de performance

### âœ… Etapa 4: OtimizaÃ§Ã£o
- Random Search com 50 iteraÃ§Ãµes
- 5-fold cross-validation
- ComparaÃ§Ã£o de 5 modelos
- AvaliaÃ§Ã£o final no conjunto de teste

### âœ… Etapa 5: DocumentaÃ§Ã£o
- RelatÃ³rio tÃ©cnico completo
- README e documentaÃ§Ã£o
- Pronto para apresentaÃ§Ã£o

---

## ğŸ’¡ RecomendaÃ§Ãµes de Uso

### Para InstituiÃ§Ãµes Educacionais

```
1. Sistema de Alertas AutomÃ¡ticos
   â””â”€ Identificar estudantes em risco (prediÃ§Ã£o < -1.5)

2. Tutoria Direcionada
   â””â”€ Focar em: notas baixas + poucas horas de estudo

3. Monitoramento de FrequÃªncia
   â””â”€ Alerta se frequÃªncia < 75%

4. Acompanhamento ContÃ­nuo
   â””â”€ Comparar prediÃ§Ã£o vs desempenho real
```

---

## ğŸ› ï¸ Como Usar o Modelo Treinado

```python
import joblib
import pandas as pd

# Carregar modelo
modelo = joblib.load('models/modelo_final.pkl')

# Preparar novos dados (deve ter as mesmas features)
novos_dados = pd.DataFrame({
    'study_hours_week': [15],
    'attendance_rate': [85],
    'previous_scores': [7.5],
    # ... outras features ...
})

# Fazer prediÃ§Ã£o
predicao = modelo.predict(novos_dados)
print(f"Nota final predita: {predicao[0]:.2f}")
```

---

## ğŸ“Š VisualizaÃ§Ãµes Geradas

O projeto gera diversos grÃ¡ficos:

- âœ… DistribuiÃ§Ã£o da variÃ¡vel alvo
- âœ… Matriz de correlaÃ§Ãµes
- âœ… PrediÃ§Ãµes vs Valores Reais
- âœ… DistribuiÃ§Ã£o de ResÃ­duos
- âœ… ImportÃ¢ncia de Features
- âœ… ComparaÃ§Ã£o de Modelos
- âœ… AnÃ¡lise de Erros por Faixa

---

## ğŸ“‹ LimitaÃ§Ãµes e Trabalhos Futuros

### LimitaÃ§Ãµes Atuais

- Dificuldade em prever notas extremas
- Dataset moderado (2.510 amostras)
- Sem features temporais
- NÃ£o captura fatores contextuais

### Trabalhos Futuros

- [ ] Deep Learning (Redes Neurais)
- [ ] Features Temporais (progresso semestral)
- [ ] Ensemble AvanÃ§ado (Stacking)
- [ ] SHAP values (interpretabilidade)
- [ ] API REST (ProduÃ§Ã£o)
- [ ] Re-treinamento AutomÃ¡tico

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo LICENSE para detalhes.

---

## ğŸ“ Contato e Suporte

Para dÃºvidas, sugestÃµes ou reportar problemas:

1. Abra uma **Issue** no GitHub
2. Consulte o **RelatÃ³rio Completo** em `docs/RELATORIO_FINAL.md`
3. Revise a documentaÃ§Ã£o em `notebooks/`

---

## ğŸ“– DocumentaÃ§Ã£o Adicional

- ğŸ“„ [RelatÃ³rio TÃ©cnico Completo](docs/RELATORIO_FINAL.md)
- ğŸ“Š [ExploraÃ§Ã£o de Dados](notebooks/01_EDA.ipynb)
- ğŸ”§ [PrÃ©-processamento](notebooks/02_Preprocessamento.ipynb)
- ğŸ¤– [Modelagem Baseline](notebooks/03_Etapa3_Baseline.ipynb)
- âš¡ [OtimizaÃ§Ã£o](notebooks/04_Etapa4_Otimizacao.ipynb)

---

## ğŸŒŸ Destaques

â­ **84% de accuracy** (RÂ² = 0.84)  
â­ **40% menos erro** em relaÃ§Ã£o ao baseline  
â­ **Modelo robusto** sem overfitting significativo  
â­ **Pronto para produÃ§Ã£o**  

---

**Status:** âœ… Projeto Completo e Documentado

```
Ãšltima atualizaÃ§Ã£o: 02/12/2025
VersÃ£o: 1.0
```